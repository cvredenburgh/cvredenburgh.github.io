[
  {
    "id": "distillation-info-theory",
    "title": "What Model Distillation Teaches Us About Representation Learning",
    "description": "Inspired by work on a ‚Äúnew information theory‚Äù of deep learning, this project investigates model distillation as an experimental lens on representation learning. Specifically, I probe questions around how compressing a large model into a smaller one reshapes the information content, sufficiency, and invariances embedded in its learned representations?",
    "tags": [
      "AI",
      "LLM",
      "distillation",
      "representation learning",
      "fine-tuning",
      "compression",
      "generalization",
      "information theory"
    ],
    "githubUrl": "https://github.com/cvredenburgh/distillation-info-theory",
    "date": "2025-10-25T00:00:00.000Z",
    "content": "<div class=\"prose prose-lg max-w-none dark:prose-invert\"><h1>Model Distillation as Information Bottleneck</h1>\n<p>üöß In progress</p>\n<h2>Background</h2>\n<h2>Hypotheses</h2>\n<h3>Methods</h3>\n<h2>Results</h2>\n<h2>Summary</h2>\n</div>",
    "slug": "distillation-info-theory"
  },
  {
    "id": "topic-modeling-llms",
    "title": "Statistical Topics to Actionable Insights: Elevating Topic Modeling With LLMs",
    "description": "Topic models can filter for meaningful signal in unstructured text, but it often stops at short n-grams or sets of words. Here I show the utility of coupling LLMs with topic modeling, turning statistical patterns into clear insights, summaries, and (potentially) adatpive workflow automation.",
    "tags": [
      "topic-modeling",
      "llm",
      "data-science",
      "insight-generation",
      "unstructured-data",
      "automation"
    ],
    "githubUrl": "https://github.com/cvredenburgh/topic-modeling-with-llms",
    "date": "2025-09-29T00:00:00.000Z",
    "content": "<div class=\"prose prose-lg max-w-none dark:prose-invert\"><h1>Statistical Topics to Actionable Insights: Elevating Topic Modeling With LLMs</h1>\n<p>üöß In progress</p>\n<h2>Topic Modeling Background</h2>\n<ul>\n<li>Why topic modeling matters in the age of LLMs</li>\n<li>Statistical foundations</li>\n<li>Limits of topic modeling</li>\n</ul>\n<h3>Applications</h3>\n<ul>\n<li>Social media</li>\n<li>Business operations</li>\n</ul>\n<h3>Popular APIs</h3>\n<ul>\n<li>BERTopic</li>\n<li>FASTopic</li>\n</ul>\n<h2>Methods</h2>\n<h3>Evaluation &amp; Optimization</h3>\n<h3>Hierarchical Topic Trends &amp; Visualizing Relationships</h3>\n<ul>\n<li>How topics evolve across time (e.g., seasonal complaints, feature shifts).</li>\n<li>Identifying parent/child topic hierarchies.</li>\n<li>Statistical significance tests on topic prevalence across groups (A/B, region, demographics).</li>\n</ul>\n<h2>LLM Topic Enrichment</h2>\n<ul>\n<li>Summarization</li>\n<li>Automated Labeling</li>\n<li>Contextual enrichment</li>\n</ul>\n<h2>Results</h2>\n<h3>Unstructured Information -&gt; Signal -&gt; Action</h3>\n<ul>\n<li>Applying statistical testing to validate observed trends.</li>\n<li>Turning noisy text into actionable KPIs (e.g., which customer segment mentions durability most).\nDetecting weak signals or emerging trends.</li>\n</ul>\n<h2>Appendix</h2>\n</div>",
    "slug": "topic-modeling-llms"
  },
  {
    "id": "fine-grained-representations",
    "title": "Regional Contrastive Learning for Fine-grained Product Representations",
    "description": "How can we teach AI to better understand the subtle differences in product design‚Äîlike the cut of a collar or the curve of a sole? This project builds on regional contrastive learning methods (E¬≤ CLiP) to refine multimodal product representations, experimenting with token alignment, pruning, and alternative transformer architectures (e.g. wavelet-based) to capture fine-grained product features. Then, I demonstrate how these representations can be leveraged for generative optimization as well as analytical outputs.",
    "tags": [
      "AI",
      "contrastive learning",
      "CLiP",
      "vision-language models",
      "product concept evaluation",
      "multimodal",
      "representation learning"
    ],
    "githubUrl": "https://github.com/cvredenburgh/multimodal-concept-evaluation",
    "date": "2025-07-25T00:00:00.000Z",
    "content": "<div class=\"prose prose-lg max-w-none dark:prose-invert\"><h1>Regional Contrastive Learning for Fine-grained Product Representations &amp; Evaluation</h1>\n<p>üöß In progress ‚Äì coming soon.</p>\n<h2>VLM &amp; CLiP Background</h2>\n<h2>Overview of Fine-Grained Representation Learning</h2>\n<h3>Background on E^2 CLiP Extension</h3>\n<h2>Applying E^2 Fork to New Domains</h2>\n<h2>Hypotheses: Constructive E^2 Extensions</h2>\n<ul>\n<li>Hierarchical specialized tokens (data quality impact)</li>\n<li>Information-based token pruning</li>\n<li>Multi-view image consistency</li>\n<li>Loosening token-to-region mapping</li>\n<li>Wavelet-based transformer for finer image representation</li>\n<li>Applying E^2 with other multimodal models</li>\n</ul>\n<h2>Methods</h2>\n<h2>Results</h2>\n<h2>Summary</h2>\n<h2>Appendix</h2>\n<p>D. Qi, H. Zhao, and S. Li, ‚ÄòEasy Regional Contrastive Learning of Expressive Fashion Representations‚Äô, presented at the The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024</p>\n</div>",
    "slug": "fine-grained-representations"
  },
  {
    "id": "hiring-active-investing",
    "title": "Hiring is Active Investing: Reflecting on the Limits of Interviews",
    "description": "Similar to active investing, a wide body of research (both academic and corporate) shows hiring managers rarely outperform the index (chance). For example, case study evaluations and coding evaluation scores consistently show very poor predictability for later job performance. I explore what productive interview cycles could instead focus on (for both sides).",
    "tags": [
      "hiring",
      "talent management",
      "interviewing"
    ],
    "githubUrl": "https://github.com/cvredenburgh/hiring-active-investing",
    "date": "2025-07-25T00:00:00.000Z",
    "content": "<div class=\"prose prose-lg max-w-none dark:prose-invert\"><p>üöß Write-up in progress</p>\n<h1>Hiring is Active Investing: Reflecting on the Limits of Interviews</h1>\n<p>A somewhat contrarian take on modern American interview practices that is supported by a wealth of evidence, both corporate and academic.</p>\n<h2>The Team Is Every Organization&#39;s Most Important Product</h2>\n<p>It may sound cliche in 2025, but it must be emphasized at the top that every organization&#39;s team is the most important product.  In a way, the team shapes the company&#39;s destiny.</p>\n<h2>A Brief History of Active Investing</h2>\n<ul>\n<li>Index investing versus consistent active performance (Benjamin Graham to Vanguard and recent hedge fund performance)</li>\n</ul>\n<h2>A Brief Overview of Interviews And Their Relative Effectiveness</h2>\n<ul>\n<li>Generally, interviews aim to filter candidate pools to identify the candidates who will perform the best on the job.</li>\n<li>Hiring is an investment in talent, aiming to maximize positive impact on the company.</li>\n<li>While not perfect, we are now awash in job performance data as well as content from interviews (structured and unstructured).</li>\n</ul>\n<h3>Unstructured Interviews</h3>\n<ul>\n<li>Generally, unstructured interviews have poor predictability of job performance (e.g. promotions, review ratings, tenure, etc.).</li>\n</ul>\n<h3>Structured Interviews</h3>\n<ul>\n<li>Perhaps less well appreciated, structured interviews also have very poor predictability for performance.</li>\n<li>Case studies also have very poor job performance predictability</li>\n<li>Coding interviews (Google studies) (high false negative and near 0 predictability)</li>\n</ul>\n<h3>False Negatives</h3>\n<ul>\n<li>Very, very few companies or managers track even basic hiring performance metrics.</li>\n<li><ul>\n<li>For example, False Negatives are rarely tracked (and are hard to account for - cite Rabois, others)</li>\n</ul>\n</li>\n</ul>\n<h3>Hiring Managers Appear to be Analogous to Active Investors</h3>\n<h2>So What To Do?</h2>\n<p>We may be in the circumstance (systemically speaking, at least in the United States) of: \na) knowing the importance of selecting a great team but \nb) not having effective tools to do so, and \nc) spending enormous resources on interviewing so as to at least <em>give the impression we are taking team development very seriously and instituting all the best (but ineffective) practices.</em></p>\n<h3>What Can Be Learned In Interview?</h3>\n<ul>\n<li><em>The candidate</em> versus interview particulars or nuances</li>\n<li>Accomplishments versus artifacts of interview structure</li>\n</ul>\n<h4>What Interviews Incentivize</h4>\n<ul>\n<li>Avoid poor incentives (interviews have been shown to incentivize overclaims, for example)</li>\n</ul>\n<h2>Takeaways</h2>\n<h2>Appendix</h2>\n<p>Levashina, J., Hartwell, C. J., Morgeson, F. P., &amp; Campion, M. A. (2014). The structured employment interview: Narrative and quantitative review. Personnel Psychology, 67(1), 241‚Äì293. (Meta on deception; ties to low r=0.18 for unstructured.)</p>\n<p>Wingate, T. G., Bourdage, J. S., &amp; Steel, P. (2024). Evaluating interview criterion-related validity for distinct constructs: A meta-analysis. International Journal of Selection and Assessment. DOI: 10.1111/ijsa.12494. (85 studies, n=30,000+; r=0.30 overall, but drops for contextual performance.)</p>\n<p>Weekley &amp; Gier (1987) within McDaniel: r=0.48 for sales (case-analog), but n=104 limits generalizability. Critique: Tan, H. (2023). Do consulting case study interviews work? A deep dive. StrategyU Blog. <a href=\"https://strategyu.co/case-studies-deep-dive/\">https://strategyu.co/case-studies-deep-dive/</a></p>\n<p>(code interviews and brain teasers near 0 predictability, 10y of Google data)\nBock, L. (2015). Work Rules! Insights from Inside Google That Will Transform How You Live and Lead.</p>\n</div>",
    "slug": "hiring-active-investing"
  },
  {
    "id": "vlm-causal-inference",
    "title": "Causal Learning From Fine-Grained VLM Representations",
    "description": "This project investigates how fine-grained representations from vision-language models (VLMs) can support causal reasoning in product design. Specifically, learning how design features impact consumer/user response. I test this by combining latent-space counterfactual probing and generative interventional loops, aiming to demonstrate how localized design features causally influence customer sentiment and desirability.",
    "tags": [
      "AI",
      "VLM",
      "causal modeling",
      "foundation models",
      "counterfactuals",
      "product concept evaluation",
      "representation learning"
    ],
    "githubUrl": "https://github.com/cvredenburgh/representation-mining",
    "date": "2025-07-25T00:00:00.000Z",
    "content": "<div class=\"prose prose-lg max-w-none dark:prose-invert\"><h1>Causal Learning From Fine-Grained VLM Representations</h1>\n<p>üöß In progress</p>\n<h2>Background</h2>\n<p>Vision-language models (VLMs), such as E¬≤-CLIP learn rich, fine-grained representations of products that encode visual and textual semantics across parts and attributes.<br>This project explores how these representations can move beyond correlation‚Äîtoward <em>causal understanding</em> of how specific visual or design elements drive consumer response.<br>The focus is on footwear concepts, where region-level embeddings capture attributes like outsole geometry, materials, and construction style.</p>\n<hr>\n<h2>Hypotheses</h2>\n<ol>\n<li><p><em>Latent Causality Hypothesis</em> ‚Äî<br>Product attributes that are <em>causally relevant</em> to customer sentiment correspond to consistent linear directions in the model‚Äôs latent space.<br>‚Üí Testable via <em>Latent-Space Counterfactual Probing (LS-CCP)</em>.</p>\n</li>\n<li><p><em>On-Manifold Intervention Hypothesis</em> ‚Äî<br>Realistic generative edits that modify only a single part or attribute (e.g., toe box material) will induce predictable and stable shifts in predicted desirability.<br>‚Üí Testable via <em>Diffusion-based Generative Interventional Optimization (DECO)</em> that leverages our evaluation model outputs for feedback.</p>\n</li>\n</ol>\n<hr>\n<h2>Methods</h2>\n<h3>1. Latent-Space Counterfactual Probing (LS-CCP)</h3>\n<ul>\n<li>Extract part-aware embeddings ( r_1, ‚Ä¶, r_M ) from a fine-tuned E¬≤ VLM.  </li>\n<li>Compute gradients and Concept Activation Vectors (CAVs) for key attributes (e.g., mesh upper, chunky sole).  </li>\n<li>Perturb embeddings along concept directions and record ‚àÜ≈∑ (change in predicted sentiment).  </li>\n<li>Aggregate these directional effects to estimate part-level Average Causal Effects (ACE).</li>\n</ul>\n<p><strong>Tools:</strong> PyTorch, Captum (Integrated Gradients), SHAP, TCAV.</p>\n<hr>\n<h3>2. Diffusion-based Generative Interventional Optimization (DECO)</h3>\n<ul>\n<li>Use diffusion models (e.g., SDXL + ControlNet) to in-paint or edit localized shoe regions conditioned on text prompts (‚Äúsame shoe, with knit upper‚Äù).  </li>\n<li>Re-evaluate each generated variant through the evaluation head trained on customer response.  </li>\n<li>Estimate causal Œî by comparing baseline vs. edited prediction; optimize edits that maximize desirability or minimize uncertainty.</li>\n</ul>\n<p><strong>Tools:</strong> Stable Diffusion, ControlNet, CLIPScore validation, PyTorch regression head.</p>\n<hr>\n<h2>Results</h2>\n<p>üöß In progress ‚Äî </p>\n<hr>\n<h2>Summary</h2>\n<p>By integrating <em>latent-space probing</em> with <em>on-manifold generative interventions</em>, this work builds a bridge between modern foundation models and causal inference in product design.<br>The ultimate goal is to move from ‚Äúfeature importance‚Äù to <em>actionable causal insight</em>‚Äîidentifying which components of a product truly drive consumer preference and how they can be optimized.</p>\n<hr>\n</div>",
    "slug": "vlm-causal-inference"
  }
]